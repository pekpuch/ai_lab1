{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A4ZDDKBhYH_u"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import opendatasets as od\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numba import cuda\n",
    "from torch.utils.data import Dataset,DataLoader,Subset,random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGxR7KhA9esG",
    "outputId": "85e1d019-f0ac-4d6f-874d-a2d14567cc43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  pekpuch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Kaggle Key:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the-simpsons-characters-dataset.zip to .\\the-simpsons-characters-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.08G/1.08G [14:43<00:00, 1.31MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "no__WJ2KZUT0"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform, target_transform=None):\n",
    "      self.img_labels = os.listdir(path = img_dir)\n",
    "      self.labels_map = {}\n",
    "      it = 0\n",
    "      for i in self.img_labels:\n",
    "        self.labels_map[i] = it\n",
    "        it+=1\n",
    "      self.labels = []\n",
    "      self.img_list = []\n",
    "      for i in self.img_labels:\n",
    "        img_dir_tmp = img_dir + '/' + i + '/'\n",
    "        for j in os.listdir(path = img_dir_tmp):\n",
    "          self.img_list.append(img_dir_tmp+j)\n",
    "          self.labels.append(i)\n",
    "      self.labels_final = []\n",
    "      for i in self.labels:\n",
    "        self.labels_final.append(self.labels_map[i])\n",
    "      self.transform = transform\n",
    "      self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "      return len(self.img_list)\n",
    "    def __getitem__(self, idx):\n",
    "      img_path = self.img_list[idx]\n",
    "      image = Image.open(img_path).convert('RGB')\n",
    "      label = self.labels_final[idx]\n",
    "      w, h = image.size\n",
    "      if self.transform:\n",
    "        image = self.transform(image)\n",
    "      return image, label\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, labels, test_dir, transform):\n",
    "      self.labels_map = labels\n",
    "      self.test_labels_map = {}\n",
    "      self.test_img_list_dir = os.listdir(path = test_dir)\n",
    "      self.test_paths = []\n",
    "      for i in self.test_img_list_dir:\n",
    "        path = '/content/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset/' + i\n",
    "        self.test_paths.append(path)\n",
    "        i = i.rstrip(\"jpg\")\n",
    "        indx = len(i) - 1\n",
    "        while (i[indx].isalpha() != True):\n",
    "          i = i[:-1]\n",
    "          indx-=1\n",
    "        self.test_labels_map[path] = self.labels_map[i]\n",
    "      self.transform = transform\n",
    "    def __len__(self):\n",
    "      return len(self.test_paths)\n",
    "    def __getitem__(self, idx):\n",
    "      img_path = self.test_paths[idx]\n",
    "      image = Image.open(img_path).convert('RGB')\n",
    "      label = self.test_labels_map[img_path]\n",
    "      w, h = image.size\n",
    "      if self.transform:\n",
    "        image = self.transform(image)\n",
    "      return image, label\n",
    "\n",
    "\n",
    "transforms = v2.Compose([\n",
    "      v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "      transforms.ToTensor(),\n",
    "      v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hYMeownwksfr"
   },
   "outputs": [],
   "source": [
    "myDataset = CustomImageDataset (img_dir = 'the-simpsons-characters-dataset/simpsons_dataset/simpsons_dataset', transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fjkj9-5NpZcY"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, labels_count = len(myDataset.labels_map)):\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    # Сверточный слой 1\n",
    "    # Этот слой применит 32 сверточных фильтра размером 3x3 на входное изображение.\n",
    "    # Шаг равен 1, что означает, что фильтр будет двигаться на 1 пиксель за раз.\n",
    "    # Заполнение равно 1, чтобы сохранить пространственные размеры (высоту и ширину) неизменными после операции свертки.\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    # Нормализация пакетов слоя 1\n",
    "    # Этот слой нормализует активации нейронов в предыдущем слое, что может помочь ускорить обучение и уменьшить чувствительность к инициализации сети.\n",
    "    self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "\n",
    "    # Слой максимального пулинга 1\n",
    "    # Этот слой уменьшит пространственные размеры (высоту и ширину) входного объема, выбрав максимальное значение в каждом патче 2x2.\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # Сверточный слой 2\n",
    "    # Этот слой применит 64 сверточных фильтра размером 3x3 на выход предыдущего слоя.\n",
    "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    # Нормализация пакетов слоя 2\n",
    "    self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    # Слой максимального пулинга 2\n",
    "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # Сверточный слой 3\n",
    "    # Этот слой применит 128 сверточных фильтров размером 3x3 на выход предыдущего слоя.\n",
    "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    # Нормализация пакетов слоя 3\n",
    "    self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    # Слой максимального пулинга 3\n",
    "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # Сверточный слой 4\n",
    "    # Этот слой применит 256 сверточных фильтров размером 3x3 на выход предыдущего слоя.\n",
    "    self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    # Нормализация пакетов слоя 4\n",
    "    self.batch_norm4 = nn.BatchNorm2d(256)\n",
    "\n",
    "    # Слой максимального пулинга 4\n",
    "    self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # Полносвязный слой 1\n",
    "    # Этот слой соединит каждый нейрон в предыдущем слое со всеми нейронами в этом слое.\n",
    "    # Он примет сплошной выход из предыдущего слоя (256 * 14 * 14) и выдаст 512 значений.\n",
    "    self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "\n",
    "    # Полносвязный слой 2\n",
    "    # Этот слой примет выход из предыдущего слоя (512) и выдаст число значений, равное числу меток.\n",
    "    self.fc2 = nn.Linear(512, labels_count)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Применить первый набор операций свертки, нормализации пакетов и максимального пулинга, за которыми следует функция активации ReLU.\n",
    "    x = self.pool1(F.relu(self.batch_norm1(self.conv1(x))))\n",
    "    x = self.pool2(F.relu(self.batch_norm2(self.conv2(x))))\n",
    "    x = self.pool3(F.relu(self.batch_norm3(self.conv3(x))))\n",
    "    x = self.pool4(F.relu(self.batch_norm4(self.conv4(x))))\n",
    "\n",
    "    # Преобразовать выход из предыдущего слоя в единичный вектор.\n",
    "    x = x.view(-1, 256 * 14 * 14)\n",
    "\n",
    "    # Применить первый полносвязный слой, за которым следует функция активации ReLU.\n",
    "    x = F.relu(self.fc1(x))\n",
    "\n",
    "    # Применить второй полносвязный слой. Это выдаст вектор вероятностей для каждой метки.\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    # Вернуть выход.\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4ZcMMy3iaGbJ"
   },
   "outputs": [],
   "source": [
    "dataset_size = len(myDataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "train_dataset, val_dataset = random_split(myDataset, [train_size, val_size]) # Разделение Train датасета на Val и Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JgVBCEA8bsBG"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=myDataset, batch_size = batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "65DPQRZME7jb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EjsrZD7PhyUI"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9,weight_decay = 0.01) # w = w - α * ∇L\n",
    "criterion = nn.CrossEntropyLoss() # L = -1/N * Σ (y_i * log(p_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YrAyU9NMGAvH"
   },
   "outputs": [],
   "source": [
    "def model_training(model,train_loader,optimizer,criterion):\n",
    "  running_loss = 0.0\n",
    "  model.train()\n",
    "  for inputs,labels in train_loader:\n",
    "    optimizer.zero_grad() # обнуление градиентов\n",
    "    inputs,labels = inputs.to(device) , labels.to(device)\n",
    "    out = model(inputs) # проход батча через слои нейронной сети\n",
    "    loss = criterion(out,labels) # подсчет потерь\n",
    "    loss.backward() # вычисление градиентов\n",
    "    optimizer.step() # обновление градиентов модели в стороны минимизации потерь\n",
    "    running_loss += loss.item()\n",
    "\n",
    "  return running_loss / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ilaAveF8GEk8"
   },
   "outputs": [],
   "source": [
    "def model_validating(model,val_loader,criterion):\n",
    "  running_loss = 0.0\n",
    "  model.eval()\n",
    "  for inputs,labels in val_loader:    \n",
    "    with torch.no_grad():\n",
    "      inputs,labels = inputs.to(device),labels.to(device)\n",
    "      out = model(inputs)\n",
    "      loss = criterion(out,labels)\n",
    "      running_loss += loss.item()\n",
    "  return running_loss / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "_svEN6GTE0x8",
    "outputId": "1d60ba5c-74ce-40d9-97b0-e49099772442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch [1/20], Train Loss: 6.1572, Val Loss: 0.7980\n",
      "Epoch [2/20], Train Loss: 5.0274, Val Loss: 0.7218\n",
      "Epoch [3/20], Train Loss: 4.6501, Val Loss: 0.6803\n",
      "Epoch [4/20], Train Loss: 4.3419, Val Loss: 0.6310\n",
      "Epoch [5/20], Train Loss: 4.0688, Val Loss: 0.6154\n",
      "Epoch [6/20], Train Loss: 3.8671, Val Loss: 0.5782\n",
      "Epoch [7/20], Train Loss: 3.7409, Val Loss: 0.5474\n",
      "Epoch [8/20], Train Loss: 3.5536, Val Loss: 0.5286\n",
      "Epoch [9/20], Train Loss: 3.4463, Val Loss: 0.5214\n",
      "Epoch [10/20], Train Loss: 3.3817, Val Loss: 0.4878\n",
      "Epoch [11/20], Train Loss: 3.2335, Val Loss: 0.4821\n",
      "Epoch [12/20], Train Loss: 3.1691, Val Loss: 0.4603\n",
      "Epoch [13/20], Train Loss: 3.1192, Val Loss: 0.4894\n",
      "Epoch [14/20], Train Loss: 3.0315, Val Loss: 0.4711\n",
      "Epoch [15/20], Train Loss: 2.9535, Val Loss: 0.4640\n",
      "Epoch [16/20], Train Loss: 2.8945, Val Loss: 0.4260\n",
      "Epoch [17/20], Train Loss: 2.8022, Val Loss: 0.4263\n",
      "Epoch [18/20], Train Loss: 2.7603, Val Loss: 0.4227\n",
      "Epoch [19/20], Train Loss: 2.7450, Val Loss: 0.4143\n",
      "Epoch [20/20], Train Loss: 2.6731, Val Loss: 0.4060\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "train_loss_arr = []\n",
    "val_loss_arr = []\n",
    "print(device)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_training(model, train_loader, optimizer, criterion)\n",
    "    val_loss = model_validating(model, val_loader, criterion)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    train_loss_arr.append(train_loss)\n",
    "    val_loss_arr.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "JxdTwRBa7rGj",
    "outputId": "c07a9087-05ba-4dd1-8a7b-7f61c8fd07c9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "x = list(range(1, 21))\n",
    "plt.plot(x, train_loss_arr)\n",
    "plt.show()\n",
    "plt.plot(x, val_loss_arr)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-UGMZfdgFzC",
    "outputId": "5908fa21-ae87-48b0-fb92-35e6b7493911"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TestDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testDataset \u001b[38;5;241m=\u001b[39m TestDataset (labels \u001b[38;5;241m=\u001b[39m myDataset\u001b[38;5;241m.\u001b[39mlabels_map, test_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset\u001b[39m\u001b[38;5;124m'\u001b[39m, transform \u001b[38;5;241m=\u001b[39m transforms)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TestDataset' is not defined"
     ]
    }
   ],
   "source": [
    "testDataset = TestDataset (labels = myDataset.labels_map, test_dir = '/content/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset', transform = transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81uwEicpL7Gq"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, '/content/model_v4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fnk14-1MSce"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/content/model_v4.pth',map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GXzJ_2nNCOO"
   },
   "outputs": [],
   "source": [
    "def predict_image(model,img_path):\n",
    "  with open(img_path,'rb') as f :\n",
    "    img = Image.open(f).convert(\"RGB\")\n",
    "  img_tensor = transforms(img).unsqueeze(0).to(device)\n",
    "  out = model(img_tensor)\n",
    "  pred_label = torch.argmax(torch.softmax(out,dim=1),dim=1)\n",
    "  for key,value in myDataset.labels_map.items():\n",
    "    if value==pred_label:\n",
    "      class_name = key\n",
    "  #print(f\"картинка {img_path} \\nрезультат предсказания - {class_name}.\")\n",
    "  return pred_label,class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJj0gid_NePM",
    "outputId": "f89ead64-bd82-4256-b010-c87ddf351c08"
   },
   "outputs": [],
   "source": [
    "predict_image(model,'/content/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset/bart_simpson_34.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3_202wkN377"
   },
   "outputs": [],
   "source": [
    "def test_testset(path):\n",
    "  path = path+'/'\n",
    "  results = []\n",
    "  test_classes_names=[]\n",
    "  model.eval()\n",
    "  for i in os.listdir(path):\n",
    "    pred_label,class_name = predict_image(model,os.path.join(path,i))\n",
    "    p=i[::-1]\n",
    "    _, p =p.split('_', 1)\n",
    "    p=p[::-1]\n",
    "    if class_name not in test_classes_names:\n",
    "        test_classes_names.append(class_name)\n",
    "    results.append((p, class_name))\n",
    "  return results , test_classes_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lLhf2ooONfd"
   },
   "outputs": [],
   "source": [
    "results,_ = test_testset('/content/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdcWl2E_-VQ-"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['Expected', 'Predicted'])\n",
    "\n",
    "df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLF0cL3P-V53"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "id": "kou0VDcw-Xv9",
    "outputId": "18e4e323-e852-44a9-8825-219d62ab1c1b"
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(df['Expected'],df['Predicted'])\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7UW1Ia5-Y2Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yutjtKOE-aRf"
   },
   "outputs": [],
   "source": [
    "true_labels = df['Expected'].values\n",
    "pred_labels = df['Predicted'].values\n",
    "cm_array = confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nLWLK29-b82",
    "outputId": "2a5438f9-7919-4abb-ee69-7f9ddbe94581"
   },
   "outputs": [],
   "source": [
    "precision = cm_array.diagonal() / cm_array.sum(axis=0)\n",
    "recall = cm_array.diagonal() / cm_array.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2zSQjkP-c7M",
    "outputId": "f1b25757-90d6-46f2-d573-8301db4df012"
   },
   "outputs": [],
   "source": [
    "pres_sum = 0\n",
    "rec_sum = 0\n",
    "f1_sum = 0\n",
    "pres_count = 0\n",
    "rec_count = 0\n",
    "f1_count = 0\n",
    "\n",
    "for i in range(len(recall)):\n",
    "  if round(precision[i],2)!=0:\n",
    "    print(f\"Class {ct.columns[i]}: Precision={round(precision[i],2)}, Recall={round(recall[i],2)} f1-score = {round(2 * ((recall[i]*precision[i])/(recall[i]+precision[i])),2)}\")\n",
    "    pres_sum += round(precision[i],2)\n",
    "    rec_sum += round(recall[i],2)\n",
    "    f1_sum += round(2 * ((recall[i]*precision[i])/(recall[i]+precision[i])),2)\n",
    "    pres_count += 1\n",
    "    rec_count += 1\n",
    "    f1_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zcig4q3s-d0q",
    "outputId": "7c473a51-c620-4cdd-f48a-e7385666f5e0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"average Precision={round(pres_sum/pres_count, 2)} average Recall={round(rec_sum/rec_count, 2)} average F1-score = { round(f1_sum/f1_count, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
